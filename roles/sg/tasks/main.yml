---
# 1) Discover VPC by tags
- name: Get VPC information by tags
  amazon.aws.ec2_vpc_net_info:
    region: "{{ aws_region }}"
    filters:
      "tag:Environment": "{{ env_name }}"
      "tag:ManagedBy": "ansible"
  register: vpc_info

- name: Fail if VPC not found
  fail:
    msg: "VPC with Environment tag '{{ env_name }}' not found. Run VPC role first."
  when: vpc_info.vpcs | length == 0

- name: Set VPC ID
  set_fact:
    vpc_id: "{{ vpc_info.vpcs[0].id }}"

# 2) Look up existing SGs by name (scoped by VPC)
- name: Lookup Control Plane SG by name
  amazon.aws.ec2_security_group_info:
    region: "{{ aws_region }}"
    filters:
      group-name: "{{ eks_controlplane_sg_name }}"
      vpc-id: "{{ vpc_id }}"
  register: cp_sg_info

- name: Lookup Worker SG by name
  amazon.aws.ec2_security_group_info:
    region: "{{ aws_region }}"
    filters:
      group-name: "{{ eks_workernodes_sg_name }}"
      vpc-id: "{{ vpc_id }}"
  register: worker_sg_info

# 3) Control Plane SG — create if missing (with rules),
#    else just ensure basic properties without touching rules/descriptions.
- name: Create EKS Control Plane Security Group (first run)
  amazon.aws.ec2_security_group:
    name: "{{ eks_controlplane_sg_name }}"
    description: "EKS Control Plane Security Group"
    vpc_id: "{{ vpc_id }}"
    region: "{{ aws_region }}"
    purge_rules: no
    rules: "{{ control_plane_rules }}"
    tags:
      Name: "{{ eks_controlplane_sg_name }}"
      Environment: "{{ env_name }}"
      ManagedBy: "ansible"
  register: eks_controlplane_sg_created
  when: cp_sg_info.security_groups | length == 0

- name: Ensure Control Plane SG basic properties (no rule updates)
  amazon.aws.ec2_security_group:
    group_id: "{{ cp_sg_info.security_groups[0].group_id }}"
    name: "{{ eks_controlplane_sg_name }}"
    description: "EKS Control Plane Security Group"
    vpc_id: "{{ vpc_id }}"
    region: "{{ aws_region }}"
    purge_rules: no # don't pass rules; avoids description update issues
    tags:
      Name: "{{ eks_controlplane_sg_name }}"
      Environment: "{{ env_name }}"
      ManagedBy: "ansible"
  register: eks_controlplane_sg_exists
  when: cp_sg_info.security_groups | length > 0

# Normalize control plane SG id into a single fact
- name: Set control plane SG id fact
  set_fact:
    eks_controlplane_sg_id: >-
      {{ (cp_sg_info.security_groups[0].group_id
          if cp_sg_info.security_groups | length > 0
          else eks_controlplane_sg_created.group_id) }}

# Optional: wait until control-plane SG is visible (eventual consistency)
- name: Wait until control-plane SG is discoverable
  amazon.aws.ec2_security_group_info:
    region: "{{ aws_region }}"
    filters:
      group-id: "{{ eks_controlplane_sg_id }}"
  register: cp_wait
  retries: 6
  delay: 5
  until: cp_wait.security_groups | length > 0

# 4) Worker SG — create with base rules if missing; otherwise ensure properties
- name: Create EKS Worker Node Security Group (base, first run)
  amazon.aws.ec2_security_group:
    name: "{{ eks_workernodes_sg_name }}"
    description: "EKS Worker Nodes Security Group"
    vpc_id: "{{ vpc_id }}"
    region: "{{ aws_region }}"
    purge_rules: no
    rules:
      - proto: all
        group_id: "{{ eks_controlplane_sg_id }}"
        rule_desc: "Allow all from control plane"
      - proto: tcp
        from_port: 30000
        to_port: 32767
        cidr_ip: "{{ vpc_cidr }}"
        rule_desc: "K8s NodePort range"
      - proto: tcp
        ports: [443]
        cidr_ip: "0.0.0.0/0"
        rule_desc: "HTTPS"
      - proto: tcp
        ports: [80]
        cidr_ip: "0.0.0.0/0"
        rule_desc: "HTTP"
    tags:
      Name: "{{ eks_workernodes_sg_name }}"
      Environment: "{{ env_name }}"
      ManagedBy: "ansible"
  register: eks_workernodes_sg_created
  when: worker_sg_info.security_groups | length == 0

- name: Ensure Worker SG basic properties (no rule updates)
  amazon.aws.ec2_security_group:
    group_id: "{{ worker_sg_info.security_groups[0].group_id }}"
    name: "{{ eks_workernodes_sg_name }}"
    description: "EKS Worker Nodes Security Group"
    vpc_id: "{{ vpc_id }}"
    region: "{{ aws_region }}"
    purge_rules: no # no "rules" here
    tags:
      Name: "{{ eks_workernodes_sg_name }}"
      Environment: "{{ env_name }}"
      ManagedBy: "ansible"
  register: eks_workernodes_sg_exists
  when: worker_sg_info.security_groups | length > 0

# Normalize worker SG id into a single fact
- name: Set worker SG id fact
  set_fact:
    eks_workernodes_sg_id: >-
      {{ (worker_sg_info.security_groups[0].group_id
          if worker_sg_info.security_groups | length > 0
          else eks_workernodes_sg_created.group_id) }}

# 5) Wait until the Worker SG is visible (eventual consistency)
- name: Wait until worker SG is discoverable
  amazon.aws.ec2_security_group_info:
    region: "{{ aws_region }}"
    filters:
      group-id: "{{ eks_workernodes_sg_id }}"
  register: sg_lookup
  retries: 6
  delay: 5
  until: sg_lookup.security_groups | length > 0

# 6) Append self-referencing rule (allow all between worker nodes)
- name: Allow all between worker nodes (self)
  amazon.aws.ec2_security_group:
    group_id: "{{ eks_workernodes_sg_id }}" # modify existing SG by ID
    name: "{{ eks_workernodes_sg_name }}" # required by module (state=present)
    description: "EKS Worker Nodes Security Group"
    vpc_id: "{{ vpc_id }}"
    region: "{{ aws_region }}"
    purge_rules: no
    rules:
      - proto: all
        group_id: "{{ eks_workernodes_sg_id }}"
        rule_desc: "Allow all between worker nodes"

# 7) Export for downstream roles
- name: Export SG IDs as facts (cacheable)
  set_fact:
    eks_controlplane_sg_id: "{{ eks_controlplane_sg_id }}"
    eks_workernodes_sg_id: "{{ eks_workernodes_sg_id }}"
    cacheable: true

# 8) Debug
- name: Debug created security groups
  debug:
    msg:
      - "Control Plane SG: {{ eks_controlplane_sg_id }}"
      - "Worker Nodes SG: {{ eks_workernodes_sg_id }}"
