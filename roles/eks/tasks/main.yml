---
# --- Auto-discover missing inputs from upstream roles ---

# 0a) Ensure we know the VPC (if vpc_id missing)
- name: Discover VPC by tags if vpc_id is missing
  amazon.aws.ec2_vpc_net_info:
    region: "{{ aws_region }}"
    filters:
      "tag:Environment": "{{ env_name }}"
      "tag:ManagedBy": "ansible"
  register: eks_vpc_info
  when: vpc_id is not defined

- name: Set vpc_id from discovery
  set_fact:
    vpc_id: "{{ eks_vpc_info.vpcs[0].id }}"
  when:
    - vpc_id is not defined
    - eks_vpc_info.vpcs | length > 0

# 0b) Discover Control Plane SG ID if missing
- name: Discover control-plane SG if missing
  amazon.aws.ec2_security_group_info:
    region: "{{ aws_region }}"
    filters:
      group-name: "{{ eks_controlplane_sg_name }}"
      vpc-id: "{{ vpc_id }}"
  register: eks_cp_sg_info
  when: eks_controlplane_sg_id is not defined

- name: Set control-plane SG id from discovery
  set_fact:
    eks_controlplane_sg_id: "{{ eks_cp_sg_info.security_groups[0].group_id }}"
  when:
    - eks_controlplane_sg_id is not defined
    - eks_cp_sg_info.security_groups | length > 0

# 0c) Discover Worker SG ID if missing (not strictly needed for cluster, but needed for LT)
- name: Discover worker SG if missing
  amazon.aws.ec2_security_group_info:
    region: "{{ aws_region }}"
    filters:
      group-name: "{{ eks_workernodes_sg_name }}"
      vpc-id: "{{ vpc_id }}"
  register: eks_worker_sg_info
  when: eks_workernodes_sg_id is not defined

- name: Set worker SG id from discovery
  set_fact:
    eks_workernodes_sg_id: "{{ eks_worker_sg_info.security_groups[0].group_id }}"
  when:
    - eks_workernodes_sg_id is not defined
    - eks_worker_sg_info.security_groups | length > 0

# 0d) Discover IAM role ARNs if missing
- name: Discover cluster role ARN if missing
  amazon.aws.iam_role_info:
    name: "{{ eks_cluster_role_name | default('eks-cluster-role-' ~ env_name) }}"
  register: eks_cluster_role_info
  when: eks_cluster_role_arn is not defined

- name: Set cluster role ARN from discovery
  set_fact:
    eks_cluster_role_arn: "{{ eks_cluster_role_info.iam_roles[0].arn }}"
  when:
    - eks_cluster_role_arn is not defined
    - eks_cluster_role_info.iam_roles | length > 0

- name: Discover nodegroup role ARN if missing
  amazon.aws.iam_role_info:
    name: "{{ eks_nodegroup_role_name | default('eks-nodegroup-role-' ~ env_name) }}"
  register: eks_nodegroup_role_info
  when: eks_nodegroup_role_arn is not defined

- name: Set nodegroup role ARN from discovery
  set_fact:
    eks_nodegroup_role_arn: "{{ eks_nodegroup_role_info.iam_roles[0].arn }}"
  when:
    - eks_nodegroup_role_arn is not defined
    - eks_nodegroup_role_info.iam_roles | length > 0

- name: Discover private subnets by tags if missing
  amazon.aws.ec2_vpc_subnet_info:
    region: "{{ aws_region }}"
    filters:
      vpc-id: "{{ vpc_id }}"
      "tag:Tier": "private" # adjust if your tag key/value differs
  register: eks_private_subnets
  when: private_subnet_ids is not defined

- name: Set private_subnet_ids from discovery
  set_fact:
    private_subnet_ids: "{{ eks_private_subnets.subnets | map(attribute='id') | list }}"
  when:
    - private_subnet_ids is not defined
    - eks_private_subnets.subnets | length > 0

# 0) Pre-flight: make sure upstream facts exist
- name: Assert inputs from VPC/SG/IAM roles
  assert:
    that:
      - private_subnet_ids is defined
      - private_subnet_ids | length > 0
      - eks_controlplane_sg_id is defined
      - eks_workernodes_sg_id is defined
      - eks_cluster_role_arn is defined
      - eks_nodegroup_role_arn is defined
    fail_msg: >
      Missing required inputs from upstream roles. Ensure 'vpc', 'sg', and 'iam'
      run before 'eks' in the same play.

###########################################
# 1) Create / Update EKS Cluster
###########################################
- name: Create / Update EKS Cluster
  community.aws.eks_cluster:
    name: "{{ eks_cluster_name }}"
    region: "{{ aws_region }}"
    version: "{{ eks_version }}"
    role_arn: "{{ eks_cluster_role_arn }}"
    subnets: "{{ private_subnet_ids }}"
    security_groups:
      - "{{ eks_controlplane_sg_id }}"
    tags: "{{ eks_cluster_tags }}"
    wait: true
    wait_timeout: "{{ cluster_create_timeout }}"
    state: present
  register: eks_cluster

###########################################
# 2) Create / Update Launch Template (attach Worker SG)
###########################################
- name: Create / Update Launch Template for Node Group (no SSH, Session Manager only)
  amazon.aws.ec2_launch_template:
    name: "lt-{{ nodegroup_name }}"
    region: "{{ aws_region }}"
    state: present
    instance_type: "{{ nodegroup_instance_types[0] }}" # move instance type here
    network_interfaces:
      - device_index: 0
        delete_on_termination: true
        groups:
          - "{{ eks_workernodes_sg_id }}"
    block_device_mappings: # move disk size here
      - device_name: /dev/xvda
        ebs:
          volume_size: "{{ nodegroup_disk_size }}"
          volume_type: gp3
          delete_on_termination: true
    # DO NOT set image_id â€” let EKS pick its managed AMI (ami_type removed below)
    tag_specifications:
      - resource_type: instance
        tags: "{{ nodegroup_tags }}"
      - resource_type: volume
        tags: "{{ nodegroup_tags }}"
  register: lt_result

###########################################
# 3) Create / Update EKS Managed Node Group
###########################################
- name: Create / Update EKS Node Group
  community.aws.eks_nodegroup:
    region: "{{ aws_region }}"
    cluster_name: "{{ eks_cluster_name }}"
    name: "{{ nodegroup_name }}"
    node_role_arn: "{{ eks_nodegroup_role_arn }}"
    subnets: "{{ private_subnet_ids }}"
    launch_template:
      name: "lt-{{ nodegroup_name }}"
      version: "$Latest"
    scaling_config:
      min_size: "{{ nodegroup_min_size }}"
      max_size: "{{ nodegroup_max_size }}"
      desired_size: "{{ nodegroup_desired_size }}"
    # REMOVE: instance_types, disk_size, ami_type (they conflict with launch_template)
    capacity_type: "{{ nodegroup_capacity_type }}"
    labels: "{{ nodegroup_labels }}"
    taints: "{{ nodegroup_taints }}"
    tags: "{{ nodegroup_tags }}"
    wait: true
    wait_timeout: "{{ nodegroup_create_timeout }}"
    state: present
  register: eks_nodegroup

###########################################
# 4) Helpful Output
###########################################
- name: Debug EKS Artifacts
  debug:
    msg:
      - "Cluster: {{ eks_cluster_name }} / Version: {{ eks_version }}"
      - "Control Plane SG: {{ eks_controlplane_sg_id }}"
      - "Worker SG: {{ eks_workernodes_sg_id }}"
      - "Private Subnets: {{ private_subnet_ids | join(', ') }}"
      - "Cluster Role: {{ eks_cluster_role_arn }}"
      - "Nodegroup Role: {{ eks_nodegroup_role_arn }}"
      - "Nodegroup: {{ nodegroup_name }} (desired={{ nodegroup_desired_size }})"
